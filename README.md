## BERT-Based Semantic Evolution Framework

This repository implements the methodology described in "Uncovering Evolving Criminal Jargon: A BERT-Based Semantic Evolution in Telegram Marketplaces"

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (recommended)
- 16GB+ RAM
- 50GB+ storage space

### Setup
```bash
git clone https://github.com/Manikdeep/BERT-TermEvolution.git
cd BERT-TermEvolution
python -m venv bert_evolution_env
source bert_evolution_env/bin/activate  # On Windows: bert_evolution_env\Scripts\activate
```

### Dependencies
```bash
# Core dependencies
pip install torch torchvision torchaudio transformers datasets tokenizers

# For CUDA support (optional but recommended)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For visualization and analysis
pip install matplotlib seaborn plotly networkx community

# For semantic analysis components
pip install nltk scikit-learn pandas numpy pathlib pickle-mixin
```

### NLTK Data Setup

```bash
  python -c "import nltk; nltk.download('stopwords')"
```

### Data Structure

Organize your data and outputs in the following structure:

```bash
BERT-TermEvolution/
├── data/
│   ├── raw/
│   │   ├── 2023-01/
│   │   ├── 2023-02/
│   │   └── ...
│   └── processed/
├── embeddings/              # Generated by BERT training notebooks
│   ├── 2023-01/
│   │   └── word_to_avg_emb.pkl
│   ├── 2023-02/
│   │   └── word_to_avg_emb.pkl
│   └── ...
├── JSON-Monthly/             # Tokenized corpus files
│   ├── 2023-01/
│   │   └── tokenized_corpus_2023-01.pkl
│   ├── 2023-02/
│   │   └── tokenized_corpus_2023-02.pkl
│   └── ...
├── results/
│   ├── networks/
│   ├── drift_plots/
│   └── analysis/
└── notebooks/
    ├── CooccurenceAnalysis.py
    └── SemanticDrift.py### Pipeline Components

#### 1. Core BERT Training

    BERT-TermEvolution-Part1.ipynb: Initial incremental training
    BERT-TermEvolution-Part2.ipynb: Continued training and embedding generation

#### 2. Co-occurrence Network Analysis Setup
File: Co-occurenceAnalysis.ipynb


#### Prerequisites:

    1. Complete BERT training notebooks to generate embedding files
    2. Have tokenized corpus files in monthly folders

#### Configuration Steps:

    1. Open Co-occurenceAnalysis.py
    2. Update file paths where necessary to match your directory structure
    3. Modify the target terms list with your terms of interest
    4. Adjust analysis parameters as needed

#### Run:
Execute all cells in the notebook sequentially

#### 3. Semantic Drift Analysis Setup
File: SemanticDriftAnalysis.ipynb
    
#### Prerequisites:

    1. Word embedding files from BERT training
    2. Monthly embedding pickle files in correct format

#### Configuration Steps:

    1. Open SemanticDriftAnalysis.ipynb
    2. Update file paths where necessary to match your directory structure
    3. Modify the analysis terms list with terms you want to analyze
    4. Adjust output file paths and parameters as needed

#### Run:
Execute all cells in the notebook sequentially

### Pre-Run Checklist

#### Before Running Co-occurrence Analysis:

 - Embeddings directory exists with monthly subfolders
 - Each month folder contains word_to_avg_emb.pkl
 - Tokenized corpus directory exists with monthly subfolders
 - Each month folder contains tokenized_corpus_YYYY-MM.pkl
 - Paths updated in script
 - Target terms defined

#### Before Running Drift Analysis:

 - Embeddings directory properly structured
 - Monthly embedding pickle files present
 - Paths updated in script
 - Analysis terms defined
 - Output directory exists

### Expected File Outputs

#### Co-occurrence Analysis:

    Network visualization PNG files
    Console output with network statistics

#### Drift Analysis:

    volatility.tsv: Term volatility rankings
    word_monthly_vecs.pkl: Monthly vector data
    Matplotlib plots showing semantic drift over time

### Data Requirements

Due to ethical considerations, actual criminal communication datasets are not provided. Users must obtain data following legal and ethical guidelines and format according to expected JSON structure.

### Complete Workflow

- Setup Environment: Install dependencies and configure paths
- Data Preparation: Organize monthly datasets in required structure
- BERT Training: Execute Part1 and Part2 notebooks
- Configure Analysis Scripts: Update paths and parameters
- Run Co-occurrence Analysis: Generate semantic networks
- Run Drift Analysis: Track temporal evolution
- Review Outputs: Examine generated visualizations and data files
```

