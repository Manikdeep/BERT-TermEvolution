{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04681e4d-810a-444b-a01d-1d2b64ba9168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaurm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from dateutil import parser\n",
    "import wandb\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM,\n",
    "    BertModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bdb6be-3717-4af5-85aa-3301645bcdf6",
   "metadata": {},
   "source": [
    "#### Data Extraction & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57203ece-cb9b-484d-834b-fa032370bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed Characters\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess(group_name, sender, text):\n",
    "    group_name = str(group_name).lower()\n",
    "    sender = str(sender).lower()\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove non-alphanumeric (but keep spaces)\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Include group_name, sender as the first tokens\n",
    "    return [group_name, sender] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eba2da25-a8ea-4bbb-a7fd-e9585c127028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_jsons_in_folder(folder_path):\n",
    "\n",
    "    all_messages = []\n",
    "    \n",
    "    # Gather all .json files\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                group_name = data.get('name', 'unknown')\n",
    "                \n",
    "                messages = data.get('messages', [])\n",
    "                for message in messages:\n",
    "                    sender = message.get('from', 'unknown')\n",
    "                    text_content = message.get('text', '')\n",
    "                    \n",
    "                    # text might be a list\n",
    "                    if isinstance(text_content, list):\n",
    "                        text_content = \" \".join(\n",
    "                            part['text'] if isinstance(part, dict) else part\n",
    "                            for part in text_content\n",
    "                        )\n",
    "                    \n",
    "                    if text_content:\n",
    "                        all_messages.append({\n",
    "                            'name': group_name,\n",
    "                            'from': sender,\n",
    "                            'text': text_content\n",
    "                        })\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error with {file_path}: {e}\")\n",
    "\n",
    "    # Build DataFrame\n",
    "    if not all_messages:\n",
    "        print(f\"No usable messages in folder: {folder_path}\")\n",
    "        return None, []\n",
    "\n",
    "    df = pd.DataFrame(all_messages)\n",
    "    \n",
    "    # Preprocess each row: add tokenized text in new column\n",
    "    df['Tokenized_Text'] = df.apply(\n",
    "        lambda row: preprocess(row['name'], row['from'], row['text']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Build a \"corpus\" (list of token lists)\n",
    "    corpus = df['Tokenized_Text'].tolist()\n",
    "    \n",
    "    print(f\"Folder: {folder_path} -> {len(df)} messages loaded.\")\n",
    "    return df, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6cb27-3e79-4c1d-aa5d-4ba63b3d94f1",
   "metadata": {},
   "source": [
    "#### Dataset for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51001973-c741-425b-821e-e823b721e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Use NLTK's sentence tokenizer\n",
    "        self.sentences = sent_tokenize(text)\n",
    "\n",
    "        # Tokenize all at once\n",
    "        self.inputs = self.tokenizer(\n",
    "            self.sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            is_split_into_words=False\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs.input_ids[idx]\n",
    "        attention_mask = self.inputs.attention_mask[idx]\n",
    "\n",
    "        # For masked LM, labels are the same as input_ids before random masking\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772afb4-ec65-4057-af99-2036a998e821",
   "metadata": {},
   "source": [
    "#### Incremental Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24faa7d3-d1eb-455e-b375-6ac3139d943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_train_bert(\n",
    "    parent_directory_path,\n",
    "    output_root_dir=\"./incremental_model_checkpoints\",\n",
    "    base_model_name='bert-base-uncased',\n",
    "    num_train_epochs=3,\n",
    "    batch_size=16\n",
    "):\n",
    "    import os\n",
    "    import pickle\n",
    "    import wandb\n",
    "    from transformers import (\n",
    "        BertTokenizer,\n",
    "        BertForMaskedLM,\n",
    "        DataCollatorForLanguageModeling,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "    from torch.utils.data import Dataset\n",
    "    import torch\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    # nltk.download('punkt')\n",
    "\n",
    "    # Gather subfolders (months)\n",
    "    subfolders = [\n",
    "        f for f in sorted(os.listdir(parent_directory_path))\n",
    "        if os.path.isdir(os.path.join(parent_directory_path, f))\n",
    "    ]\n",
    "    if not subfolders:\n",
    "        print(\"No subfolders found under:\", parent_directory_path)\n",
    "        return\n",
    "\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the base model & tokenizer\n",
    "    print(f\"Loading base model/tokenizer: {base_model_name}\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(base_model_name)\n",
    "    model = BertForMaskedLM.from_pretrained(base_model_name).to(device)\n",
    "\n",
    "    # Data collator for masked LM\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "    )\n",
    "\n",
    "    # Disable W&B (if you do not want to log)\n",
    "    wandb.init(mode=\"disabled\")\n",
    "\n",
    "    # Define a simple TextDataset class\n",
    "    class TextDataset(Dataset):\n",
    "        def __init__(self, text, tokenizer, max_length=512):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            # Split text into sentences\n",
    "            self.sentences = sent_tokenize(text)\n",
    "            \n",
    "            # Tokenize all sentences\n",
    "            self.inputs = self.tokenizer(\n",
    "                self.sentences,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                is_split_into_words=False\n",
    "            )\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs.input_ids)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            input_ids = self.inputs.input_ids[idx]\n",
    "            attention_mask = self.inputs.attention_mask[idx]\n",
    "\n",
    "            # For masked LM, labels are the same as input_ids before masking\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels\n",
    "            }\n",
    "\n",
    "    # Loop over each month folder\n",
    "    for month_name in subfolders:\n",
    "        folder_path = os.path.join(parent_directory_path, month_name)\n",
    "        print(f\"\\n=== Processing: {month_name} ===\")\n",
    "\n",
    "        # Load and Preprocess\n",
    "        df, month_corpus = load_and_preprocess_jsons_in_folder(folder_path)\n",
    "        if df is None or not month_corpus:\n",
    "            print(f\"No data in {month_name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Save the tokenized corpus (pkl) for this month\n",
    "        tokenized_outpath = os.path.join(folder_path, f\"tokenized_corpus_{month_name}.pkl\")\n",
    "        with open(tokenized_outpath, \"wb\") as f:\n",
    "            pickle.dump(month_corpus, f)\n",
    "        print(f\"Tokenized corpus saved to: {tokenized_outpath}\")\n",
    "\n",
    "        # Build one big text for BERT training\n",
    "        all_text_corpus = ' '.join([' '.join(sentence) for sentence in month_corpus]).lower()\n",
    "\n",
    "        # Create PyTorch Dataset\n",
    "        dataset = TextDataset(all_text_corpus, tokenizer, max_length=512)\n",
    "        if len(dataset) == 0:\n",
    "            print(f\"No valid sentences in {month_name}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Training dataset size for {month_name}: {len(dataset)} sentences.\")\n",
    "\n",
    "        # Training arguments\n",
    "        output_dir = os.path.join(output_root_dir, f\"checkpoint-{month_name}\")\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            save_steps=100,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=os.path.join(output_root_dir, \"logs\"),\n",
    "            logging_steps=100,\n",
    "            weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=dataset\n",
    "        )\n",
    "\n",
    "        # Train and captu\n",
    "        train_output = trainer.train()\n",
    "        final_loss = train_output.training_loss  # Average loss across training\n",
    "\n",
    "        print(f\"Finished training for {month_name}. Final training loss: {final_loss:.4f}\")\n",
    "\n",
    "        # SAVE CHECKPOINT\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Checkpoint saved: {output_dir}\")\n",
    "\n",
    "        # Reload the newly trained model for the next month\n",
    "        model = BertForMaskedLM.from_pretrained(output_dir).to(device)\n",
    "\n",
    "    print(\"\\nAll done with incremental training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c565d5-c449-4499-b073-c7d6cccc985e",
   "metadata": {},
   "source": [
    "#### Main Function (for all months in the folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6e5d2aa-f910-4ab5-8445-19980a972ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base model/tokenizer: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing: 2023-01 ===\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-01\\result - 2025-02-11T131848.154.json: Expecting value: line 5 column 15 (char 133)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-01\\result - 2025-02-11T131848.154_1.json: Expecting value: line 5 column 15 (char 133)\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-01 -> 548771 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-01\\tokenized_corpus_2023-01.pkl\n",
      "Training dataset size for 2023-01: 5535 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1038' max='1038' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1038/1038 2:49:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.848600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.633400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.551100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-01. Final training loss: 1.9498\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-01\n",
      "\n",
      "=== Processing: 2023-02 ===\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-02\\result (46).json: Expecting value: line 5 column 15 (char 96)\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-02 -> 552997 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-02\\tokenized_corpus_2023-02.pkl\n",
      "Training dataset size for 2023-02: 4039 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='759' max='759' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [759/759 2:51:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.293400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.168500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-02. Final training loss: 1.3605\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-02\n",
      "\n",
      "=== Processing: 2023-03 ===\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-03\\result (69).json: Expecting value: line 5 column 15 (char 96)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-03\\result (83).json: Expecting value: line 5 column 15 (char 89)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-03\\result_200.json: Expecting value: line 5 column 15 (char 133)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-03\\result_200_1.json: Expecting value: line 5 column 15 (char 133)\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-03 -> 760816 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-03\\tokenized_corpus_2023-03.pkl\n",
      "Training dataset size for 2023-03: 3504 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='657' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [657/657 1:59:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.707200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.649800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.630800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-03. Final training loss: 1.7782\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-03\n",
      "\n",
      "=== Processing: 2023-04 ===\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-04\\result_37.json: Expecting value: line 5 column 15 (char 94)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-04\\result_37_1.json: Expecting value: line 5 column 15 (char 94)\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-04 -> 382092 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-04\\tokenized_corpus_2023-04.pkl\n",
      "Training dataset size for 2023-04: 4242 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='798' max='798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [798/798 2:23:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.733800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.599900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.373200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-04. Final training loss: 1.5535\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-04\n",
      "\n",
      "=== Processing: 2023-05 ===\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-05\\result_101.json: Expecting value: line 5 column 15 (char 86)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-05\\result_101_1.json: Expecting value: line 5 column 15 (char 86)\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-05 -> 373781 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-05\\tokenized_corpus_2023-05.pkl\n",
      "Training dataset size for 2023-05: 3301 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='621' max='621' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [621/621 1:47:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.810600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.587400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.282100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-05. Final training loss: 1.4523\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-05\n",
      "\n",
      "=== Processing: 2023-06 ===\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-06 -> 352435 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-06\\tokenized_corpus_2023-06.pkl\n",
      "Training dataset size for 2023-06: 2664 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 1:31:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.319300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.253300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-06. Final training loss: 1.3723\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-06\n",
      "\n",
      "=== Processing: 2023-07 ===\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-07\\result (27).json: Expecting value: line 5 column 15 (char 90)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-07\\result (7).json: Expecting value: line 5 column 15 (char 85)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-07\\result_203.json: Expecting value: line 5 column 15 (char 85)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-07\\result_203_1.json: Expecting value: line 5 column 15 (char 85)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-07\\result_333.json: Expecting value: line 5 column 15 (char 94)\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-07\\result_333_1.json: Expecting value: line 5 column 15 (char 94)\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-07 -> 272111 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-07\\tokenized_corpus_2023-07.pkl\n",
      "Training dataset size for 2023-07: 3645 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='684' max='684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [684/684 2:05:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-07. Final training loss: 0.0000\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-07\n",
      "\n",
      "=== Processing: 2023-08 ===\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-08 -> 376277 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-08\\tokenized_corpus_2023-08.pkl\n",
      "Training dataset size for 2023-08: 4250 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='798' max='798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [798/798 2:42:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-08. Final training loss: 0.0000\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-08\n",
      "\n",
      "=== Processing: 2023-09 ===\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-09 -> 174390 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-09\\tokenized_corpus_2023-09.pkl\n",
      "Training dataset size for 2023-09: 2269 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='426' max='426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [426/426 1:19:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-09. Final training loss: 0.0000\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-09\n",
      "\n",
      "=== Processing: 2023-10 ===\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-10\\result (45).json: Expecting ',' delimiter: line 786 column 4 (char 18195)\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-10 -> 247677 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-10\\tokenized_corpus_2023-10.pkl\n",
      "Training dataset size for 2023-10: 4026 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 2:20:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-10. Final training loss: 0.0000\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-10\n",
      "\n",
      "=== Processing: 2023-11 ===\n",
      "Error decoding JSON from C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-11\\result (34).json: Expecting value: line 5 column 15 (char 107)\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-11 -> 242553 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-11\\tokenized_corpus_2023-11.pkl\n",
      "Training dataset size for 2023-11: 9405 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1764' max='1764' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1764/1764 5:24:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-11. Final training loss: 0.0000\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-11\n",
      "\n",
      "=== Processing: 2023-12 ===\n",
      "Folder: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-12 -> 173790 messages loaded.\n",
      "Tokenized corpus saved to: C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\\2023-12\\tokenized_corpus_2023-12.pkl\n",
      "Training dataset size for 2023-12: 1709 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/321 1:06:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-12. Final training loss: 0.0000\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-12\n",
      "\n",
      "All done with incremental training!\n"
     ]
    }
   ],
   "source": [
    "# Set your parent directory path\n",
    "parent_directory_path = \"C:/Users/kaurm/OneDrive/Desktop/Research/Paper3/Model/JSON/JSON-Monthly\"  \n",
    "\n",
    "# Run incremental training\n",
    "incremental_train_bert(\n",
    "    parent_directory_path=parent_directory_path,\n",
    "    output_root_dir=\"./incremental_checkpoints\",\n",
    "    base_model_name='bert-base-uncased',\n",
    "    num_train_epochs=3,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70539c-ec4c-48b6-b63b-300452c9f818",
   "metadata": {},
   "source": [
    "#### See Part 2 for Remaining Months and Generate Embeddings From Final Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
