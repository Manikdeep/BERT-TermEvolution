{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04681e4d-810a-444b-a01d-1d2b64ba9168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from dateutil import parser\n",
    "import wandb\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM,\n",
    "    BertModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bdb6be-3717-4af5-85aa-3301645bcdf6",
   "metadata": {},
   "source": [
    "#### Data Extraction & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57203ece-cb9b-484d-834b-fa032370bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed Characters\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess(group_name, sender, text):\n",
    "    group_name = str(group_name).lower()\n",
    "    sender = str(sender).lower()\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove non-alphanumeric (but keep spaces)\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Include group_name, sender as the first tokens\n",
    "    return [group_name, sender] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba2da25-a8ea-4bbb-a7fd-e9585c127028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_jsons_in_folder(folder_path):\n",
    "\n",
    "    all_messages = []\n",
    "    \n",
    "    # Gather all .json files\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                group_name = data.get('name', 'unknown')\n",
    "                \n",
    "                messages = data.get('messages', [])\n",
    "                for message in messages:\n",
    "                    sender = message.get('from', 'unknown')\n",
    "                    text_content = message.get('text', '')\n",
    "                    \n",
    "                    # text might be a list\n",
    "                    if isinstance(text_content, list):\n",
    "                        text_content = \" \".join(\n",
    "                            part['text'] if isinstance(part, dict) else part\n",
    "                            for part in text_content\n",
    "                        )\n",
    "                    \n",
    "                    if text_content:\n",
    "                        all_messages.append({\n",
    "                            'name': group_name,\n",
    "                            'from': sender,\n",
    "                            'text': text_content\n",
    "                        })\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error with {file_path}: {e}\")\n",
    "\n",
    "    # Build DataFrame\n",
    "    if not all_messages:\n",
    "        print(f\"No usable messages in folder: {folder_path}\")\n",
    "        return None, []\n",
    "\n",
    "    df = pd.DataFrame(all_messages)\n",
    "    \n",
    "    # Preprocess each row: add tokenized text in new column\n",
    "    df['Tokenized_Text'] = df.apply(\n",
    "        lambda row: preprocess(row['name'], row['from'], row['text']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Build a \"corpus\" (list of token lists)\n",
    "    corpus = df['Tokenized_Text'].tolist()\n",
    "    \n",
    "    print(f\"Folder: {folder_path} -> {len(df)} messages loaded.\")\n",
    "    return df, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6cb27-3e79-4c1d-aa5d-4ba63b3d94f1",
   "metadata": {},
   "source": [
    "#### Dataset for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51001973-c741-425b-821e-e823b721e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Use NLTK's sentence tokenizer\n",
    "        self.sentences = sent_tokenize(text)\n",
    "\n",
    "        # Tokenize all at once\n",
    "        self.inputs = self.tokenizer(\n",
    "            self.sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            is_split_into_words=False\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs.input_ids[idx]\n",
    "        attention_mask = self.inputs.attention_mask[idx]\n",
    "\n",
    "        # For masked LM, labels are the same as input_ids before random masking\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772afb4-ec65-4057-af99-2036a998e821",
   "metadata": {},
   "source": [
    "#### Incremental Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24faa7d3-d1eb-455e-b375-6ac3139d943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import wandb\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# If needed:\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def incremental_train_bert(\n",
    "    parent_directory_path,\n",
    "    output_root_dir=\"./incremental_model_checkpoints\",\n",
    "    base_model_name='bert-base-uncased',\n",
    "    num_train_epochs=3,\n",
    "    batch_size=16,\n",
    "    start_month_name=None,\n",
    "    initial_checkpoint_path=None\n",
    "):\n",
    "\n",
    "    # 1. Gather and sort subfolders\n",
    "    subfolders = [\n",
    "        f for f in sorted(os.listdir(parent_directory_path))\n",
    "        if os.path.isdir(os.path.join(parent_directory_path, f))\n",
    "    ]\n",
    "    if not subfolders:\n",
    "        print(\"No subfolders found under:\", parent_directory_path)\n",
    "        return\n",
    "\n",
    "    # 2. Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 3. Determine which checkpoint to load at the start\n",
    "    if initial_checkpoint_path and os.path.isdir(initial_checkpoint_path):\n",
    "        # Load from an existing checkpoint (e.g. \"checkpoint-June2023\")\n",
    "        print(f\"Loading initial model from checkpoint: {initial_checkpoint_path}\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(initial_checkpoint_path)\n",
    "        model = BertForMaskedLM.from_pretrained(initial_checkpoint_path).to(device)\n",
    "    else:\n",
    "        # Load from base model\n",
    "        print(f\"Loading base model/tokenizer: {base_model_name}\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(base_model_name)\n",
    "        model = BertForMaskedLM.from_pretrained(base_model_name).to(device)\n",
    "\n",
    "    # 4. Data collator for masked LM\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "    )\n",
    "\n",
    "    # 5. Disable W&B (if you do not want to log)\n",
    "    wandb.init(mode=\"disabled\")\n",
    "\n",
    "    # 6. TextDataset for BERT\n",
    "    class TextDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, text, tokenizer, max_length=512):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            # Split text into sentences\n",
    "            self.sentences = sent_tokenize(text)\n",
    "            \n",
    "            # Tokenize\n",
    "            self.inputs = self.tokenizer(\n",
    "                self.sentences,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                is_split_into_words=False\n",
    "            )\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs.input_ids)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            input_ids = self.inputs.input_ids[idx]\n",
    "            attention_mask = self.inputs.attention_mask[idx]\n",
    "            labels = input_ids.clone()  # For masked LM\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels\n",
    "            }\n",
    "\n",
    "    # 8. Actually loop over subfolders\n",
    "    start_collecting = True if (start_month_name is None) else False\n",
    "\n",
    "    for month_name in subfolders:\n",
    "        if not start_collecting:\n",
    "            # We haven't reached the start_month_name yet\n",
    "            if month_name == start_month_name:\n",
    "                start_collecting = True\n",
    "            else:\n",
    "                print(f\"Skipping {month_name} until we reach {start_month_name}...\")\n",
    "                continue\n",
    "\n",
    "        folder_path = os.path.join(parent_directory_path, month_name)\n",
    "        print(f\"\\n=== Processing month: {month_name} ===\")\n",
    "\n",
    "        # Load & Preprocess\n",
    "        df, month_corpus = load_and_preprocess_jsons_in_folder(folder_path)\n",
    "        if df is None or not month_corpus:\n",
    "            print(f\"No data for {month_name}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Save tokenized corpus\n",
    "        tokenized_outpath = os.path.join(folder_path, f\"tokenized_corpus_{month_name}.pkl\")\n",
    "        with open(tokenized_outpath, \"wb\") as f:\n",
    "            pickle.dump(month_corpus, f)\n",
    "        print(f\"Tokenized corpus saved to: {tokenized_outpath}\")\n",
    "\n",
    "        # Build a text for BERT\n",
    "        all_text_corpus = ' '.join([' '.join(sent) for sent in month_corpus]).lower()\n",
    "        dataset = TextDataset(all_text_corpus, tokenizer, max_length=512)\n",
    "        if len(dataset) == 0:\n",
    "            print(f\"No valid sentences in {month_name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Training dataset size for {month_name}: {len(dataset)} sentences.\")\n",
    "\n",
    "        # Training args\n",
    "        output_dir = os.path.join(output_root_dir, f\"checkpoint-{month_name}\")\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            save_steps=100,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=os.path.join(output_root_dir, \"logs\"),\n",
    "            logging_steps=100,\n",
    "            weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=dataset\n",
    "        )\n",
    "\n",
    "        train_output = trainer.train()\n",
    "        final_loss = train_output.training_loss\n",
    "\n",
    "        print(f\"Finished training for {month_name}. Final training loss: {final_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Checkpoint saved: {output_dir}\")\n",
    "\n",
    "        # Reload the newly trained model for next iteration\n",
    "        model = BertForMaskedLM.from_pretrained(output_dir).to(device)\n",
    "        print(f\"===== Done with {month_name} =====\")\n",
    "\n",
    "    print(\"\\nAll done with incremental training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c565d5-c449-4499-b073-c7d6cccc985e",
   "metadata": {},
   "source": [
    "#### Main Function (starting from a previous checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e5d2aa-f910-4ab5-8445-19980a972ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\kaurm\\AppData\\Local\\Temp\\ipykernel_44560\\2406263928.py:2: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  parent_directory_path=\"D:\\Paper3\\Model\\JSON\\JSON-Monthly\", # change the path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base model/tokenizer: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 2023-01 until we reach 2023-06...\n",
      "Skipping 2023-02 until we reach 2023-06...\n",
      "Skipping 2023-03 until we reach 2023-06...\n",
      "Skipping 2023-04 until we reach 2023-06...\n",
      "Skipping 2023-05 until we reach 2023-06...\n",
      "\n",
      "=== Processing month: 2023-06 ===\n",
      "Folder: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-06 -> 352435 messages loaded.\n",
      "Tokenized corpus saved to: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-06\\tokenized_corpus_2023-06.pkl\n",
      "Training dataset size for 2023-06: 2664 sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 38:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.889400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.719300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-06. Final training loss: 2.0976\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-06\n",
      "===== Done with 2023-06 =====\n",
      "\n",
      "=== Processing month: 2023-07 ===\n",
      "Error decoding JSON from D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-07\\result (27).json: Expecting value: line 5 column 15 (char 90)\n",
      "Error decoding JSON from D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-07\\result (7).json: Expecting value: line 5 column 15 (char 85)\n",
      "Error decoding JSON from D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-07\\result_203.json: Expecting value: line 5 column 15 (char 85)\n",
      "Error decoding JSON from D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-07\\result_203_1.json: Expecting value: line 5 column 15 (char 85)\n",
      "Error decoding JSON from D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-07\\result_333.json: Expecting value: line 5 column 15 (char 94)\n",
      "Error decoding JSON from D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-07\\result_333_1.json: Expecting value: line 5 column 15 (char 94)\n",
      "Folder: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-07 -> 272111 messages loaded.\n",
      "Tokenized corpus saved to: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-07\\tokenized_corpus_2023-07.pkl\n",
      "Training dataset size for 2023-07: 3645 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='684' max='684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [684/684 51:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.199100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-07. Final training loss: 1.3739\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-07\n",
      "===== Done with 2023-07 =====\n",
      "\n",
      "=== Processing month: 2023-08 ===\n",
      "Folder: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-08 -> 376277 messages loaded.\n",
      "Tokenized corpus saved to: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-08\\tokenized_corpus_2023-08.pkl\n",
      "Training dataset size for 2023-08: 4250 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='798' max='798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [798/798 3:43:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.290400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-08. Final training loss: 1.4712\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-08\n",
      "===== Done with 2023-08 =====\n",
      "\n",
      "=== Processing month: 2023-09 ===\n",
      "Folder: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-09 -> 174390 messages loaded.\n",
      "Tokenized corpus saved to: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-09\\tokenized_corpus_2023-09.pkl\n",
      "Training dataset size for 2023-09: 2269 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='426' max='426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [426/426 42:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.012500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-09. Final training loss: 1.1257\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-09\n",
      "===== Done with 2023-09 =====\n",
      "\n",
      "=== Processing month: 2023-10 ===\n",
      "Error decoding JSON from D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-10\\result (45).json: Expecting ',' delimiter: line 786 column 4 (char 18195)\n",
      "Folder: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-10 -> 247677 messages loaded.\n",
      "Tokenized corpus saved to: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-10\\tokenized_corpus_2023-10.pkl\n",
      "Training dataset size for 2023-10: 4026 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 57:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.975700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-10. Final training loss: 1.1273\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-10\n",
      "===== Done with 2023-10 =====\n",
      "\n",
      "=== Processing month: 2023-11 ===\n",
      "Error decoding JSON from D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-11\\result (34).json: Expecting value: line 5 column 15 (char 107)\n",
      "Folder: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-11 -> 242553 messages loaded.\n",
      "Tokenized corpus saved to: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-11\\tokenized_corpus_2023-11.pkl\n",
      "Training dataset size for 2023-11: 9405 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1764' max='1764' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1764/1764 3:00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.486400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.961700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.888800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.767400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.703300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.725100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-11. Final training loss: 0.9100\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-11\n",
      "===== Done with 2023-11 =====\n",
      "\n",
      "=== Processing month: 2023-12 ===\n",
      "Folder: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-12 -> 173790 messages loaded.\n",
      "Tokenized corpus saved to: D:\\Paper3\\Model\\JSON\\JSON-Monthly\\2023-12\\tokenized_corpus_2023-12.pkl\n",
      "Training dataset size for 2023-12: 1709 sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/321 24:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.954400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for 2023-12. Final training loss: 1.0439\n",
      "Checkpoint saved: ./incremental_checkpoints\\checkpoint-2023-12\n",
      "===== Done with 2023-12 =====\n",
      "\n",
      "All done with incremental training!\n"
     ]
    }
   ],
   "source": [
    "incremental_train_bert(\n",
    "    parent_directory_path=\"D:\\Paper3\\Model\\JSON\\JSON-Monthly\", # edit your PATH\n",
    "    output_root_dir=\"./incremental_checkpoints\",\n",
    "    base_model_name='bert-base-uncased',\n",
    "    num_train_epochs=3,\n",
    "    batch_size=16,\n",
    "    start_month_name=\"2023-06\",                          # skip subfolders before \"2023-06\"\n",
    "    initial_checkpoint_path=\"./incremental_checkpoints/checkpoint-2023-05\" # edit your PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cfd0b-76cb-47aa-9577-46e5b6487853",
   "metadata": {},
   "source": [
    "#### Generate Embeddings From Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aa969bc-fa49-42ac-aee5-9377da3cd562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 548,771 sentences\n",
      "  processed 1,000/548,771\n",
      "  processed 2,000/548,771\n",
      "  processed 3,000/548,771\n",
      "  processed 4,000/548,771\n",
      "  processed 5,000/548,771\n",
      "  processed 6,000/548,771\n",
      "  processed 7,000/548,771\n",
      "  processed 8,000/548,771\n",
      "  processed 9,000/548,771\n",
      "  processed 10,000/548,771\n",
      "  processed 11,000/548,771\n",
      "  processed 12,000/548,771\n",
      "  processed 13,000/548,771\n",
      "  processed 14,000/548,771\n",
      "  processed 15,000/548,771\n",
      "  processed 16,000/548,771\n",
      "  processed 17,000/548,771\n",
      "  processed 18,000/548,771\n",
      "  processed 19,000/548,771\n",
      "  processed 20,000/548,771\n",
      "  processed 21,000/548,771\n",
      "  processed 22,000/548,771\n",
      "  processed 23,000/548,771\n",
      "  processed 24,000/548,771\n",
      "  processed 25,000/548,771\n",
      "  processed 26,000/548,771\n",
      "  processed 27,000/548,771\n",
      "  processed 28,000/548,771\n",
      "  processed 29,000/548,771\n",
      "  processed 30,000/548,771\n",
      "  processed 31,000/548,771\n",
      "  processed 32,000/548,771\n",
      "  processed 33,000/548,771\n",
      "  processed 34,000/548,771\n",
      "  processed 35,000/548,771\n",
      "  processed 36,000/548,771\n",
      "  processed 37,000/548,771\n",
      "  processed 38,000/548,771\n",
      "  processed 39,000/548,771\n",
      "  processed 40,000/548,771\n",
      "  processed 41,000/548,771\n",
      "  processed 42,000/548,771\n",
      "  processed 43,000/548,771\n",
      "  processed 44,000/548,771\n",
      "  processed 45,000/548,771\n",
      "  processed 46,000/548,771\n",
      "  processed 47,000/548,771\n",
      "  processed 48,000/548,771\n",
      "  processed 49,000/548,771\n",
      "  processed 50,000/548,771\n",
      "  processed 51,000/548,771\n",
      "  processed 52,000/548,771\n",
      "  processed 53,000/548,771\n",
      "  processed 54,000/548,771\n",
      "  processed 55,000/548,771\n",
      "  processed 56,000/548,771\n",
      "  processed 57,000/548,771\n",
      "  processed 58,000/548,771\n",
      "  processed 59,000/548,771\n",
      "  processed 60,000/548,771\n",
      "  processed 61,000/548,771\n",
      "  processed 62,000/548,771\n",
      "  processed 63,000/548,771\n",
      "  processed 64,000/548,771\n",
      "  processed 65,000/548,771\n",
      "  processed 66,000/548,771\n",
      "  processed 67,000/548,771\n",
      "  processed 68,000/548,771\n",
      "  processed 69,000/548,771\n",
      "  processed 70,000/548,771\n",
      "  processed 71,000/548,771\n",
      "  processed 72,000/548,771\n",
      "  processed 73,000/548,771\n",
      "  processed 74,000/548,771\n",
      "  processed 75,000/548,771\n",
      "  processed 76,000/548,771\n",
      "  processed 77,000/548,771\n",
      "  processed 78,000/548,771\n",
      "  processed 79,000/548,771\n",
      "  processed 80,000/548,771\n",
      "  processed 81,000/548,771\n",
      "  processed 82,000/548,771\n",
      "  processed 83,000/548,771\n",
      "  processed 84,000/548,771\n",
      "  processed 85,000/548,771\n",
      "  processed 86,000/548,771\n",
      "  processed 87,000/548,771\n",
      "  processed 88,000/548,771\n",
      "  processed 89,000/548,771\n",
      "  processed 90,000/548,771\n",
      "  processed 91,000/548,771\n",
      "  processed 92,000/548,771\n",
      "  processed 93,000/548,771\n",
      "  processed 94,000/548,771\n",
      "  processed 95,000/548,771\n",
      "  processed 96,000/548,771\n",
      "  processed 97,000/548,771\n",
      "  processed 98,000/548,771\n",
      "  processed 99,000/548,771\n",
      "  processed 100,000/548,771\n",
      "  processed 101,000/548,771\n",
      "  processed 102,000/548,771\n",
      "  processed 103,000/548,771\n",
      "  processed 104,000/548,771\n",
      "  processed 105,000/548,771\n",
      "  processed 106,000/548,771\n",
      "  processed 107,000/548,771\n",
      "  processed 108,000/548,771\n",
      "  processed 109,000/548,771\n",
      "  processed 110,000/548,771\n",
      "  processed 111,000/548,771\n",
      "  processed 112,000/548,771\n",
      "  processed 113,000/548,771\n",
      "  processed 114,000/548,771\n",
      "  processed 115,000/548,771\n",
      "  processed 116,000/548,771\n",
      "  processed 117,000/548,771\n",
      "  processed 118,000/548,771\n",
      "  processed 119,000/548,771\n",
      "  processed 120,000/548,771\n",
      "  processed 121,000/548,771\n",
      "  processed 122,000/548,771\n",
      "  processed 123,000/548,771\n",
      "  processed 124,000/548,771\n",
      "  processed 125,000/548,771\n",
      "  processed 126,000/548,771\n",
      "  processed 127,000/548,771\n",
      "  processed 128,000/548,771\n",
      "  processed 129,000/548,771\n",
      "  processed 130,000/548,771\n",
      "  processed 131,000/548,771\n",
      "  processed 132,000/548,771\n",
      "  processed 133,000/548,771\n",
      "  processed 134,000/548,771\n",
      "  processed 135,000/548,771\n",
      "  processed 136,000/548,771\n",
      "  processed 137,000/548,771\n",
      "  processed 138,000/548,771\n",
      "  processed 139,000/548,771\n",
      "  processed 140,000/548,771\n",
      "  processed 141,000/548,771\n",
      "  processed 142,000/548,771\n",
      "  processed 143,000/548,771\n",
      "  processed 144,000/548,771\n",
      "  processed 145,000/548,771\n",
      "  processed 146,000/548,771\n",
      "  processed 147,000/548,771\n",
      "  processed 148,000/548,771\n",
      "  processed 149,000/548,771\n",
      "  processed 150,000/548,771\n",
      "  processed 151,000/548,771\n",
      "  processed 152,000/548,771\n",
      "  processed 153,000/548,771\n",
      "  processed 154,000/548,771\n",
      "  processed 155,000/548,771\n",
      "  processed 156,000/548,771\n",
      "  processed 157,000/548,771\n",
      "  processed 158,000/548,771\n",
      "  processed 159,000/548,771\n",
      "  processed 160,000/548,771\n",
      "  processed 161,000/548,771\n",
      "  processed 162,000/548,771\n",
      "  processed 163,000/548,771\n",
      "  processed 164,000/548,771\n",
      "  processed 165,000/548,771\n",
      "  processed 166,000/548,771\n",
      "  processed 167,000/548,771\n",
      "  processed 168,000/548,771\n",
      "  processed 169,000/548,771\n",
      "  processed 170,000/548,771\n",
      "  processed 171,000/548,771\n",
      "  processed 172,000/548,771\n",
      "  processed 173,000/548,771\n",
      "  processed 174,000/548,771\n",
      "  processed 175,000/548,771\n",
      "  processed 176,000/548,771\n",
      "  processed 177,000/548,771\n",
      "  processed 178,000/548,771\n",
      "  processed 179,000/548,771\n",
      "  processed 180,000/548,771\n",
      "  processed 181,000/548,771\n",
      "  processed 182,000/548,771\n",
      "  processed 183,000/548,771\n",
      "  processed 184,000/548,771\n",
      "  processed 185,000/548,771\n",
      "  processed 186,000/548,771\n",
      "  processed 187,000/548,771\n",
      "  processed 188,000/548,771\n",
      "  processed 189,000/548,771\n",
      "  processed 190,000/548,771\n",
      "  processed 191,000/548,771\n",
      "  processed 192,000/548,771\n",
      "  processed 193,000/548,771\n",
      "  processed 194,000/548,771\n",
      "  processed 195,000/548,771\n",
      "  processed 196,000/548,771\n",
      "  processed 197,000/548,771\n",
      "  processed 198,000/548,771\n",
      "  processed 199,000/548,771\n",
      "  processed 200,000/548,771\n",
      "  processed 201,000/548,771\n",
      "  processed 202,000/548,771\n",
      "  processed 203,000/548,771\n",
      "  processed 204,000/548,771\n",
      "  processed 205,000/548,771\n",
      "  processed 206,000/548,771\n",
      "  processed 207,000/548,771\n",
      "  processed 208,000/548,771\n",
      "  processed 209,000/548,771\n",
      "  processed 210,000/548,771\n",
      "  processed 211,000/548,771\n",
      "  processed 212,000/548,771\n",
      "  processed 213,000/548,771\n",
      "  processed 214,000/548,771\n",
      "  processed 215,000/548,771\n",
      "  processed 216,000/548,771\n",
      "  processed 217,000/548,771\n",
      "  processed 218,000/548,771\n",
      "  processed 219,000/548,771\n",
      "  processed 220,000/548,771\n",
      "  processed 221,000/548,771\n",
      "  processed 222,000/548,771\n",
      "  processed 223,000/548,771\n",
      "  processed 224,000/548,771\n",
      "  processed 225,000/548,771\n",
      "  processed 226,000/548,771\n",
      "  processed 227,000/548,771\n",
      "  processed 228,000/548,771\n",
      "  processed 229,000/548,771\n",
      "  processed 230,000/548,771\n",
      "  processed 231,000/548,771\n",
      "  processed 232,000/548,771\n",
      "  processed 233,000/548,771\n",
      "  processed 234,000/548,771\n",
      "  processed 235,000/548,771\n",
      "  processed 236,000/548,771\n",
      "  processed 237,000/548,771\n",
      "  processed 238,000/548,771\n",
      "  processed 239,000/548,771\n",
      "  processed 240,000/548,771\n",
      "  processed 241,000/548,771\n",
      "  processed 242,000/548,771\n",
      "  processed 243,000/548,771\n",
      "  processed 244,000/548,771\n",
      "  processed 245,000/548,771\n",
      "  processed 246,000/548,771\n",
      "  processed 247,000/548,771\n",
      "  processed 248,000/548,771\n",
      "  processed 249,000/548,771\n",
      "  processed 250,000/548,771\n",
      "  processed 251,000/548,771\n",
      "  processed 252,000/548,771\n",
      "  processed 253,000/548,771\n",
      "  processed 254,000/548,771\n",
      "  processed 255,000/548,771\n",
      "  processed 256,000/548,771\n",
      "  processed 257,000/548,771\n",
      "  processed 258,000/548,771\n",
      "  processed 259,000/548,771\n",
      "  processed 260,000/548,771\n",
      "  processed 261,000/548,771\n",
      "  processed 262,000/548,771\n",
      "  processed 263,000/548,771\n",
      "  processed 264,000/548,771\n",
      "  processed 265,000/548,771\n",
      "  processed 266,000/548,771\n",
      "  processed 267,000/548,771\n",
      "  processed 268,000/548,771\n",
      "  processed 269,000/548,771\n",
      "  processed 270,000/548,771\n",
      "  processed 271,000/548,771\n",
      "  processed 272,000/548,771\n",
      "  processed 273,000/548,771\n",
      "  processed 274,000/548,771\n",
      "  processed 275,000/548,771\n",
      "  processed 276,000/548,771\n",
      "  processed 277,000/548,771\n",
      "  processed 278,000/548,771\n",
      "  processed 279,000/548,771\n",
      "  processed 280,000/548,771\n",
      "  processed 281,000/548,771\n",
      "  processed 282,000/548,771\n",
      "  processed 283,000/548,771\n",
      "  processed 284,000/548,771\n",
      "  processed 285,000/548,771\n",
      "  processed 286,000/548,771\n",
      "  processed 287,000/548,771\n",
      "  processed 288,000/548,771\n",
      "  processed 289,000/548,771\n",
      "  processed 290,000/548,771\n",
      "  processed 291,000/548,771\n",
      "  processed 292,000/548,771\n",
      "  processed 293,000/548,771\n",
      "  processed 294,000/548,771\n",
      "  processed 295,000/548,771\n",
      "  processed 296,000/548,771\n",
      "  processed 297,000/548,771\n",
      "  processed 298,000/548,771\n",
      "  processed 299,000/548,771\n",
      "  processed 300,000/548,771\n",
      "  processed 301,000/548,771\n",
      "  processed 302,000/548,771\n",
      "  processed 303,000/548,771\n",
      "  processed 304,000/548,771\n",
      "  processed 305,000/548,771\n",
      "  processed 306,000/548,771\n",
      "  processed 307,000/548,771\n",
      "  processed 308,000/548,771\n",
      "  processed 309,000/548,771\n",
      "  processed 310,000/548,771\n",
      "  processed 311,000/548,771\n",
      "  processed 312,000/548,771\n",
      "  processed 313,000/548,771\n",
      "  processed 314,000/548,771\n",
      "  processed 315,000/548,771\n",
      "  processed 316,000/548,771\n",
      "  processed 317,000/548,771\n",
      "  processed 318,000/548,771\n",
      "  processed 319,000/548,771\n",
      "  processed 320,000/548,771\n",
      "  processed 321,000/548,771\n",
      "  processed 322,000/548,771\n",
      "  processed 323,000/548,771\n",
      "  processed 324,000/548,771\n",
      "  processed 325,000/548,771\n",
      "  processed 326,000/548,771\n",
      "  processed 327,000/548,771\n",
      "  processed 328,000/548,771\n",
      "  processed 329,000/548,771\n",
      "  processed 330,000/548,771\n",
      "  processed 331,000/548,771\n",
      "  processed 332,000/548,771\n",
      "  processed 333,000/548,771\n",
      "  processed 334,000/548,771\n",
      "  processed 335,000/548,771\n",
      "  processed 336,000/548,771\n",
      "  processed 337,000/548,771\n",
      "  processed 338,000/548,771\n",
      "  processed 339,000/548,771\n",
      "  processed 340,000/548,771\n",
      "  processed 341,000/548,771\n",
      "  processed 342,000/548,771\n",
      "  processed 343,000/548,771\n",
      "  processed 344,000/548,771\n",
      "  processed 345,000/548,771\n",
      "  processed 346,000/548,771\n",
      "  processed 347,000/548,771\n",
      "  processed 348,000/548,771\n",
      "  processed 349,000/548,771\n",
      "  processed 350,000/548,771\n",
      "  processed 351,000/548,771\n",
      "  processed 352,000/548,771\n",
      "  processed 353,000/548,771\n",
      "  processed 354,000/548,771\n",
      "  processed 355,000/548,771\n",
      "  processed 356,000/548,771\n",
      "  processed 357,000/548,771\n",
      "  processed 358,000/548,771\n",
      "  processed 359,000/548,771\n",
      "  processed 360,000/548,771\n",
      "  processed 361,000/548,771\n",
      "  processed 362,000/548,771\n",
      "  processed 363,000/548,771\n",
      "  processed 364,000/548,771\n",
      "  processed 365,000/548,771\n",
      "  processed 366,000/548,771\n",
      "  processed 367,000/548,771\n",
      "  processed 368,000/548,771\n",
      "  processed 369,000/548,771\n",
      "  processed 370,000/548,771\n",
      "  processed 371,000/548,771\n",
      "  processed 372,000/548,771\n",
      "  processed 373,000/548,771\n",
      "  processed 374,000/548,771\n",
      "  processed 375,000/548,771\n",
      "  processed 376,000/548,771\n",
      "  processed 377,000/548,771\n",
      "  processed 378,000/548,771\n",
      "  processed 379,000/548,771\n",
      "  processed 380,000/548,771\n",
      "  processed 381,000/548,771\n",
      "  processed 382,000/548,771\n",
      "  processed 383,000/548,771\n",
      "  processed 384,000/548,771\n",
      "  processed 385,000/548,771\n",
      "  processed 386,000/548,771\n",
      "  processed 387,000/548,771\n",
      "  processed 388,000/548,771\n",
      "  processed 389,000/548,771\n",
      "  processed 390,000/548,771\n",
      "  processed 391,000/548,771\n",
      "  processed 392,000/548,771\n",
      "  processed 393,000/548,771\n",
      "  processed 394,000/548,771\n",
      "  processed 395,000/548,771\n",
      "  processed 396,000/548,771\n",
      "  processed 397,000/548,771\n",
      "  processed 398,000/548,771\n",
      "  processed 399,000/548,771\n",
      "  processed 400,000/548,771\n",
      "  processed 401,000/548,771\n",
      "  processed 402,000/548,771\n",
      "  processed 403,000/548,771\n",
      "  processed 404,000/548,771\n",
      "  processed 405,000/548,771\n",
      "  processed 406,000/548,771\n",
      "  processed 407,000/548,771\n",
      "  processed 408,000/548,771\n",
      "  processed 409,000/548,771\n",
      "  processed 410,000/548,771\n",
      "  processed 411,000/548,771\n",
      "  processed 412,000/548,771\n",
      "  processed 413,000/548,771\n",
      "  processed 414,000/548,771\n",
      "  processed 415,000/548,771\n",
      "  processed 416,000/548,771\n",
      "  processed 417,000/548,771\n",
      "  processed 418,000/548,771\n",
      "  processed 419,000/548,771\n",
      "  processed 420,000/548,771\n",
      "  processed 421,000/548,771\n",
      "  processed 422,000/548,771\n",
      "  processed 423,000/548,771\n",
      "  processed 424,000/548,771\n",
      "  processed 425,000/548,771\n",
      "  processed 426,000/548,771\n",
      "  processed 427,000/548,771\n",
      "  processed 428,000/548,771\n",
      "  processed 429,000/548,771\n",
      "  processed 430,000/548,771\n",
      "  processed 431,000/548,771\n",
      "  processed 432,000/548,771\n",
      "  processed 433,000/548,771\n",
      "  processed 434,000/548,771\n",
      "  processed 435,000/548,771\n",
      "  processed 436,000/548,771\n",
      "  processed 437,000/548,771\n",
      "  processed 438,000/548,771\n",
      "  processed 439,000/548,771\n",
      "  processed 440,000/548,771\n",
      "  processed 441,000/548,771\n",
      "  processed 442,000/548,771\n",
      "  processed 443,000/548,771\n",
      "  processed 444,000/548,771\n",
      "  processed 445,000/548,771\n",
      "  processed 446,000/548,771\n",
      "  processed 447,000/548,771\n",
      "  processed 448,000/548,771\n",
      "  processed 449,000/548,771\n",
      "  processed 450,000/548,771\n",
      "  processed 451,000/548,771\n",
      "  processed 452,000/548,771\n",
      "  processed 453,000/548,771\n",
      "  processed 454,000/548,771\n",
      "  processed 455,000/548,771\n",
      "  processed 456,000/548,771\n",
      "  processed 457,000/548,771\n",
      "  processed 458,000/548,771\n",
      "  processed 459,000/548,771\n",
      "  processed 460,000/548,771\n",
      "  processed 461,000/548,771\n",
      "  processed 462,000/548,771\n",
      "  processed 463,000/548,771\n",
      "  processed 464,000/548,771\n",
      "  processed 465,000/548,771\n",
      "  processed 466,000/548,771\n",
      "  processed 467,000/548,771\n",
      "  processed 468,000/548,771\n",
      "  processed 469,000/548,771\n",
      "  processed 470,000/548,771\n",
      "  processed 471,000/548,771\n",
      "  processed 472,000/548,771\n",
      "  processed 473,000/548,771\n",
      "  processed 474,000/548,771\n",
      "  processed 475,000/548,771\n",
      "  processed 476,000/548,771\n",
      "  processed 477,000/548,771\n",
      "  processed 478,000/548,771\n",
      "  processed 479,000/548,771\n",
      "  processed 480,000/548,771\n",
      "  processed 481,000/548,771\n",
      "  processed 482,000/548,771\n",
      "  processed 483,000/548,771\n",
      "  processed 484,000/548,771\n",
      "  processed 485,000/548,771\n",
      "  processed 486,000/548,771\n",
      "  processed 487,000/548,771\n",
      "  processed 488,000/548,771\n",
      "  processed 489,000/548,771\n",
      "  processed 490,000/548,771\n",
      "  processed 491,000/548,771\n",
      "  processed 492,000/548,771\n",
      "  processed 493,000/548,771\n",
      "  processed 494,000/548,771\n",
      "  processed 495,000/548,771\n",
      "  processed 496,000/548,771\n",
      "  processed 497,000/548,771\n",
      "  processed 498,000/548,771\n",
      "  processed 499,000/548,771\n",
      "  processed 500,000/548,771\n",
      "  processed 501,000/548,771\n",
      "  processed 502,000/548,771\n",
      "  processed 503,000/548,771\n",
      "  processed 504,000/548,771\n",
      "  processed 505,000/548,771\n",
      "  processed 506,000/548,771\n",
      "  processed 507,000/548,771\n",
      "  processed 508,000/548,771\n",
      "  processed 509,000/548,771\n",
      "  processed 510,000/548,771\n",
      "  processed 511,000/548,771\n",
      "  processed 512,000/548,771\n",
      "  processed 513,000/548,771\n",
      "  processed 514,000/548,771\n",
      "  processed 515,000/548,771\n",
      "  processed 516,000/548,771\n",
      "  processed 517,000/548,771\n",
      "  processed 518,000/548,771\n",
      "  processed 519,000/548,771\n",
      "  processed 520,000/548,771\n",
      "  processed 521,000/548,771\n",
      "  processed 522,000/548,771\n",
      "  processed 523,000/548,771\n",
      "  processed 524,000/548,771\n",
      "  processed 525,000/548,771\n",
      "  processed 526,000/548,771\n",
      "  processed 527,000/548,771\n",
      "  processed 528,000/548,771\n",
      "  processed 529,000/548,771\n",
      "  processed 530,000/548,771\n",
      "  processed 531,000/548,771\n",
      "  processed 532,000/548,771\n",
      "  processed 533,000/548,771\n",
      "  processed 534,000/548,771\n",
      "  processed 535,000/548,771\n",
      "  processed 536,000/548,771\n",
      "  processed 537,000/548,771\n",
      "  processed 538,000/548,771\n",
      "  processed 539,000/548,771\n",
      "  processed 540,000/548,771\n",
      "  processed 541,000/548,771\n",
      "  processed 542,000/548,771\n",
      "  processed 543,000/548,771\n",
      "  processed 544,000/548,771\n",
      "  processed 545,000/548,771\n",
      "  processed 546,000/548,771\n",
      "  processed 547,000/548,771\n",
      "  processed 548,000/548,771\n",
      " token matrices streamed  →  embeddings/2023-01/all_word_embeddings.pkl.gz\n",
      " 42,447 word averages → embeddings/2023-01/word_to_avg_emb.pkl\n"
     ]
    }
   ],
   "source": [
    "import torch, pickle, gzip, numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizerFast, BertModel, logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def build_embeddings_stream(\n",
    "        model_dir: str,\n",
    "        corpus_pkl: str,\n",
    "        out_token_stream: str = \"all_word_embeddings.pkl.gz\",\n",
    "        out_avg_embeds:   str = \"word_to_avg_emb.pkl\",\n",
    "        log_every: int    = 1000\n",
    "    ):\n",
    "    if not torch.cuda.is_available():\n",
    "        raise EnvironmentError(\"Need a GPU to run the encoder efficiently.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    tok  = BertTokenizerFast.from_pretrained(model_dir)\n",
    "    mdl  = BertModel.from_pretrained(model_dir).to(device).eval()\n",
    "\n",
    "    MAXLEN  = mdl.config.max_position_embeddings      # 512\n",
    "    CHUNK   = MAXLEN - 2                              # room for CLS/SEP\n",
    "\n",
    "    def chunk(lst, size=CHUNK):\n",
    "        for i in range(0, len(lst), size):\n",
    "            yield lst[i:i + size]\n",
    "\n",
    "    # load tokenised corpus\n",
    "    with open(corpus_pkl, \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "    n_sent = len(corpus)\n",
    "    print(f\"Corpus: {n_sent:,} sentences\")\n",
    "\n",
    "    # open gzip stream for token-level matrices\n",
    "    Path(out_token_stream).parent.mkdir(parents=True, exist_ok=True)\n",
    "    stream = gzip.open(out_token_stream, \"wb\")\n",
    "\n",
    "    running_sum  = defaultdict(lambda: np.zeros(768, dtype=np.float32))\n",
    "    running_cnt  = defaultdict(int)\n",
    "\n",
    "    for idx, tokens in enumerate(corpus, 1):\n",
    "        sent_vecs = []\n",
    "\n",
    "        # split long sentences into ≤510-token chunks\n",
    "        for piece in chunk(tokens):\n",
    "            enc = tok(piece, return_tensors=\"pt\", is_split_into_words=True,\n",
    "                      padding=\"max_length\", truncation=True,\n",
    "                      max_length=MAXLEN).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                hidden = mdl(**enc).last_hidden_state[0]        # (512,768) # Extract from final hidden layer\n",
    "\n",
    "            real = len(piece)\n",
    "            vecs = hidden[1 : 1 + real].cpu().numpy().astype(\"float16\") # 768-dimensional vectors\n",
    "\n",
    "            sent_vecs.append(vecs)\n",
    "\n",
    "            # update running sum / count\n",
    "            for t, v in zip(piece, vecs):\n",
    "                running_sum[t.lower()] += v.astype(np.float32)\n",
    "                running_cnt[t.lower()] += 1\n",
    "\n",
    "        # stream this sentence matrix and free memory\n",
    "        pickle.dump(np.vstack(sent_vecs), stream)\n",
    "\n",
    "        if log_every and idx % log_every == 0:\n",
    "            print(f\"  processed {idx:,}/{n_sent:,}\")\n",
    "\n",
    "    stream.close()\n",
    "    print(f\" token matrices streamed  →  {out_token_stream}\")\n",
    "\n",
    "    # write word-level averages (float16)\n",
    "    word_to_avg = {w: (running_sum[w] / running_cnt[w]).astype(\"float16\")\n",
    "                   for w in running_sum}\n",
    "    with open(out_avg_embeds, \"wb\") as f:\n",
    "        pickle.dump(word_to_avg, f)\n",
    "    print(f\" {len(word_to_avg):,} word averages → {out_avg_embeds}\")\n",
    "\n",
    "\n",
    "# run per month \n",
    "build_embeddings_stream(\n",
    "    model_dir        = \"./incremental_checkpoints/checkpoint-2023-01\",\n",
    "    corpus_pkl       = \"D:/Paper3/Model/JSON/JSON-Monthly/2023-01/tokenized_corpus_2023-01.pkl\", # edit your PATH\n",
    "    out_token_stream = \"embeddings/2023-01/all_word_embeddings.pkl.gz\",\n",
    "    out_avg_embeds   = \"embeddings/2023-01/word_to_avg_emb.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3008d1d-bf58-498f-b67d-de1e0fae3e4b",
   "metadata": {},
   "source": [
    "#### Example Terms for Nearest-Neighbor Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb98cfa5-aeff-45da-bc0d-eda80964eb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded – 30,427 tokens\n",
      "\n",
      "Top 20 terms similar to 'bank':\n",
      "  credit               0.9645\n",
      "  union                0.9576\n",
      "  business             0.9539\n",
      "  one                  0.9526\n",
      "  card                 0.9525\n",
      "  any                  0.9506\n",
      "  first                0.9504\n",
      "  with                 0.9498\n",
      "  of                   0.9497\n",
      "  and                  0.9481\n",
      "  money                0.9476\n",
      "  high                 0.9465\n",
      "  wire                 0.9458\n",
      "  all                  0.9421\n",
      "  no                   0.9419\n",
      "  good                 0.9394\n",
      "  banks                0.9394\n",
      "  check                0.9391\n",
      "  pay                  0.9388\n",
      "  dm                   0.9387\n"
     ]
    }
   ],
   "source": [
    "# Utilities for nearest-neighbour lookup\n",
    "import pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "AVG_PKL = Path(\"word_to_avg_emb.pkl\")      # adjust filename if needed\n",
    "if not AVG_PKL.exists():\n",
    "    raise FileNotFoundError(f\"{AVG_PKL} not found – run Cell 1 first.\")\n",
    "\n",
    "with AVG_PKL.open(\"rb\") as f:\n",
    "    word_to_avg = pickle.load(f)\n",
    "\n",
    "vocab  = list(word_to_avg.keys())\n",
    "matrix = np.vstack([word_to_avg[w] for w in vocab])   # (V, dim)\n",
    "print(f\"Vocabulary loaded – {len(vocab):,} tokens\")\n",
    "\n",
    "def top_related(word: str, k: int = 10):\n",
    "    # Print top-k neighbours of *word*\n",
    "    w = word.lower()\n",
    "    if w not in word_to_avg:\n",
    "        print(f\"'{word}' not in vocab\"); return\n",
    "    sims  = cosine_similarity(word_to_avg[w].reshape(1,-1), matrix)[0]\n",
    "    order = sims.argsort()[::-1]\n",
    "    print(f\"\\nTop {k} terms similar to '{word}':\")\n",
    "    n = 0\n",
    "    for idx in order:\n",
    "        cand = vocab[idx]\n",
    "        if cand == w: continue\n",
    "        print(f\"  {cand:<20} {sims[idx]:.4f}\")\n",
    "        n += 1\n",
    "        if n == k: break\n",
    "\n",
    "# example\n",
    "top_related(\"bank\", k=20) # add your word and run to see top 20 related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a75c9e82-4d30-420f-8f76-81062cdc82c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded – 30,427 tokens\n",
      "\n",
      "Top 20 terms similar to 'checks':\n",
      "  slips                0.9277\n",
      "  cards                0.9065\n",
      "  and                  0.9050\n",
      "  with                 0.9037\n",
      "  dumps                0.8981\n",
      "  logs                 0.8971\n",
      "  cashapp              0.8957\n",
      "  to                   0.8931\n",
      "  pin                  0.8915\n",
      "  valid                0.8869\n",
      "  or                   0.8857\n",
      "  pay                  0.8857\n",
      "  dm                   0.8856\n",
      "  methods              0.8851\n",
      "  card                 0.8841\n",
      "  you                  0.8835\n",
      "  apple                0.8819\n",
      "  all                  0.8812\n",
      "  clone                0.8811\n",
      "  in                   0.8811\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "AVG_PKL = Path(\"word_to_avg_emb.pkl\")      # adjust filename if needed\n",
    "if not AVG_PKL.exists():\n",
    "    raise FileNotFoundError(f\"{AVG_PKL} not found – run Cell 1 first.\")\n",
    "\n",
    "with AVG_PKL.open(\"rb\") as f:\n",
    "    word_to_avg = pickle.load(f)\n",
    "\n",
    "vocab  = list(word_to_avg.keys())\n",
    "matrix = np.vstack([word_to_avg[w] for w in vocab])   # (V, dim)\n",
    "print(f\"Vocabulary loaded – {len(vocab):,} tokens\")\n",
    "\n",
    "def top_related(word: str, k: int = 10):\n",
    "    # Print top-k neighbours of *word*\n",
    "    w = word.lower()\n",
    "    if w not in word_to_avg:\n",
    "        print(f\"'{word}' not in vocab\"); return\n",
    "    sims  = cosine_similarity(word_to_avg[w].reshape(1,-1), matrix)[0]\n",
    "    order = sims.argsort()[::-1]\n",
    "    print(f\"\\nTop {k} terms similar to '{word}':\")\n",
    "    n = 0\n",
    "    for idx in order:\n",
    "        cand = vocab[idx]\n",
    "        if cand == w: continue\n",
    "        print(f\"  {cand:<20} {sims[idx]:.4f}\")\n",
    "        n += 1\n",
    "        if n == k: break\n",
    "\n",
    "# example\n",
    "top_related(\"checks\", k=20) # add your word and run to see top 20 related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bb9bc64-cd82-4d60-9cdc-0d1995e89e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded – 30,427 tokens\n",
      "\n",
      "Top 20 terms similar to 'grubs':\n",
      "  nights               0.8790\n",
      "  swire                0.8644\n",
      "  💯💯                   0.8538\n",
      "  𝙿𝚊𝚜𝚜𝚠𝚘𝚛𝚍             0.8491\n",
      "  🕷️                   0.8484\n",
      "  neo                  0.8481\n",
      "  🕷️success🕷️          0.8471\n",
      "  gens                 0.8469\n",
      "  shield               0.8458\n",
      "  agecy                0.8445\n",
      "  🥶big goat 🥶          0.8443\n",
      "  wug💥🧤                0.8431\n",
      "  checkspenfed         0.8415\n",
      "  welly💚🥶😈             0.8401\n",
      "  doncashbullet💰💎      0.8379\n",
      "  erry                 0.8376\n",
      "  qp                   0.8368\n",
      "  vy                   0.8355\n",
      "  darlington💎          0.8352\n",
      "  quagenx              0.8347\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "AVG_PKL = Path(\"word_to_avg_emb.pkl\")      # adjust filename if needed\n",
    "if not AVG_PKL.exists():\n",
    "    raise FileNotFoundError(f\"{AVG_PKL} not found – run Cell 1 first.\")\n",
    "\n",
    "with AVG_PKL.open(\"rb\") as f:\n",
    "    word_to_avg = pickle.load(f)\n",
    "\n",
    "vocab  = list(word_to_avg.keys())\n",
    "matrix = np.vstack([word_to_avg[w] for w in vocab])   # (V, dim)\n",
    "print(f\"Vocabulary loaded – {len(vocab):,} tokens\")\n",
    "\n",
    "def top_related(word: str, k: int = 10):\n",
    "    # Print top-k neighbours of *word*\n",
    "    w = word.lower()\n",
    "    if w not in word_to_avg:\n",
    "        print(f\"'{word}' not in vocab\"); return\n",
    "    sims  = cosine_similarity(word_to_avg[w].reshape(1,-1), matrix)[0]\n",
    "    order = sims.argsort()[::-1]\n",
    "    print(f\"\\nTop {k} terms similar to '{word}':\")\n",
    "    n = 0\n",
    "    for idx in order:\n",
    "        cand = vocab[idx]\n",
    "        if cand == w: continue\n",
    "        print(f\"  {cand:<20} {sims[idx]:.4f}\")\n",
    "        n += 1\n",
    "        if n == k: break\n",
    "\n",
    "# example\n",
    "top_related(\"grubs\", k=20) # add your word and run to see top 20 related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd1336ae-2bbc-4b6d-b484-e88e09089ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded – 30,427 tokens\n",
      "\n",
      "Top 20 terms similar to 'drops':\n",
      "  pnc                  0.9106\n",
      "  instant              0.9105\n",
      "  wells                0.9042\n",
      "  federal              0.9029\n",
      "  chase                0.9025\n",
      "  boa                  0.9015\n",
      "  navy                 0.8977\n",
      "  cu                   0.8969\n",
      "  aged                 0.8945\n",
      "  usaa                 0.8918\n",
      "  wire                 0.8918\n",
      "  unions               0.8914\n",
      "  week                 0.8912\n",
      "  near                 0.8901\n",
      "  pm                   0.8897\n",
      "  asap                 0.8894\n",
      "  old                  0.8862\n",
      "  report               0.8859\n",
      "  let                  0.8853\n",
      "  banks                0.8853\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "AVG_PKL = Path(\"word_to_avg_emb.pkl\")      # adjust filename if needed\n",
    "if not AVG_PKL.exists():\n",
    "    raise FileNotFoundError(f\"{AVG_PKL} not found – run Cell 1 first.\")\n",
    "\n",
    "with AVG_PKL.open(\"rb\") as f:\n",
    "    word_to_avg = pickle.load(f)\n",
    "\n",
    "vocab  = list(word_to_avg.keys())\n",
    "matrix = np.vstack([word_to_avg[w] for w in vocab])   # (V, dim)\n",
    "print(f\"Vocabulary loaded – {len(vocab):,} tokens\")\n",
    "\n",
    "def top_related(word: str, k: int = 10):\n",
    "    # Print top-k neighbours of *word*\n",
    "    w = word.lower()\n",
    "    if w not in word_to_avg:\n",
    "        print(f\"'{word}' not in vocab\"); return\n",
    "    sims  = cosine_similarity(word_to_avg[w].reshape(1,-1), matrix)[0]\n",
    "    order = sims.argsort()[::-1]\n",
    "    print(f\"\\nTop {k} terms similar to '{word}':\")\n",
    "    n = 0\n",
    "    for idx in order:\n",
    "        cand = vocab[idx]\n",
    "        if cand == w: continue\n",
    "        print(f\"  {cand:<20} {sims[idx]:.4f}\")\n",
    "        n += 1\n",
    "        if n == k: break\n",
    "\n",
    "# example\n",
    "top_related(\"drops\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80c8ae95-e0db-485b-a08a-c78927984efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded – 30,427 tokens\n",
      "\n",
      "Top 20 terms similar to 'CC':\n",
      "  pay                  0.9476\n",
      "  and                  0.9476\n",
      "  apple                0.9414\n",
      "  fullz                0.9408\n",
      "  all                  0.9397\n",
      "  cashapp              0.9341\n",
      "  cards                0.9340\n",
      "  with                 0.9327\n",
      "  logs                 0.9303\n",
      "  dm                   0.9295\n",
      "  for                  0.9279\n",
      "  to                   0.9278\n",
      "  usa                  0.9264\n",
      "  bank                 0.9211\n",
      "  fresh                0.9208\n",
      "  in                   0.9196\n",
      "  available            0.9191\n",
      "  uk                   0.9167\n",
      "  are                  0.9160\n",
      "  card                 0.9157\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "AVG_PKL = Path(\"word_to_avg_emb.pkl\")      # adjust filename if needed\n",
    "if not AVG_PKL.exists():\n",
    "    raise FileNotFoundError(f\"{AVG_PKL} not found – run Cell 1 first.\")\n",
    "\n",
    "with AVG_PKL.open(\"rb\") as f:\n",
    "    word_to_avg = pickle.load(f)\n",
    "\n",
    "vocab  = list(word_to_avg.keys())\n",
    "matrix = np.vstack([word_to_avg[w] for w in vocab])   # (V, dim)\n",
    "print(f\"Vocabulary loaded – {len(vocab):,} tokens\")\n",
    "\n",
    "def top_related(word: str, k: int = 10):\n",
    "    # Print top-k neighbours of *word*\n",
    "    w = word.lower()\n",
    "    if w not in word_to_avg:\n",
    "        print(f\"'{word}' not in vocab\"); return\n",
    "    sims  = cosine_similarity(word_to_avg[w].reshape(1,-1), matrix)[0]\n",
    "    order = sims.argsort()[::-1]\n",
    "    print(f\"\\nTop {k} terms similar to '{word}':\")\n",
    "    n = 0\n",
    "    for idx in order:\n",
    "        cand = vocab[idx]\n",
    "        if cand == w: continue\n",
    "        print(f\"  {cand:<20} {sims[idx]:.4f}\")\n",
    "        n += 1\n",
    "        if n == k: break\n",
    "\n",
    "# example\n",
    "top_related(\"CC\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa21ef12-0721-455c-a238-b7778239dcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded – 30,427 tokens\n",
      "\n",
      "Top 20 terms similar to 'fullz':\n",
      "  cc                   0.9408\n",
      "  bin                  0.9335\n",
      "  uk                   0.9191\n",
      "  logs                 0.9156\n",
      "  bank                 0.9145\n",
      "  card                 0.9130\n",
      "  dl                   0.9128\n",
      "  pay                  0.9127\n",
      "  bins                 0.9119\n",
      "  cards                0.9080\n",
      "  type                 0.9035\n",
      "  apple                0.9030\n",
      "  pin                  0.9023\n",
      "  and                  0.9019\n",
      "  usa                  0.9016\n",
      "  pros                 0.8983\n",
      "  with                 0.8982\n",
      "  payment              0.8977\n",
      "  first                0.8973\n",
      "  high                 0.8972\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "AVG_PKL = Path(\"word_to_avg_emb.pkl\")      # adjust filename if needed\n",
    "if not AVG_PKL.exists():\n",
    "    raise FileNotFoundError(f\"{AVG_PKL} not found – run Cell 1 first.\")\n",
    "\n",
    "with AVG_PKL.open(\"rb\") as f:\n",
    "    word_to_avg = pickle.load(f)\n",
    "\n",
    "vocab  = list(word_to_avg.keys())\n",
    "matrix = np.vstack([word_to_avg[w] for w in vocab])   # (V, dim)\n",
    "print(f\"Vocabulary loaded – {len(vocab):,} tokens\")\n",
    "\n",
    "def top_related(word: str, k: int = 10):\n",
    "    # Print top-k neighbours of *word*\n",
    "    w = word.lower()\n",
    "    if w not in word_to_avg:\n",
    "        print(f\"'{word}' not in vocab\"); return\n",
    "    sims  = cosine_similarity(word_to_avg[w].reshape(1,-1), matrix)[0]\n",
    "    order = sims.argsort()[::-1]\n",
    "    print(f\"\\nTop {k} terms similar to '{word}':\")\n",
    "    n = 0\n",
    "    for idx in order:\n",
    "        cand = vocab[idx]\n",
    "        if cand == w: continue\n",
    "        print(f\"  {cand:<20} {sims[idx]:.4f}\")\n",
    "        n += 1\n",
    "        if n == k: break\n",
    "\n",
    "# example\n",
    "top_related(\"fullz\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da25024-efeb-4888-9f25-b01b4592a6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
